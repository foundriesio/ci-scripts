#!/usr/bin/python3

from concurrent.futures import ThreadPoolExecutor
import json
import os
import io
import pty
import sys
from tempfile import mkstemp
import requests
import tarfile
from shutil import rmtree
from typing import List

from helpers import (
    Progress,
    cmd,
    fio_dnsbase,
    generate_credential_tokens,
    load_extra_certs,
    require_env,
    require_secrets,
    secret,
    http_get,
    status,
)
from static_deltas import Delta, generate_deltas


class ProgressCb:
    def __init__(self, total_length: 0):
        self.total_length = total_length
        self.total_written = 0
        self.next_percent = 5

    def cb(self, written: int):
        self.total_written += written
        percent = round(self.total_written / self.total_length * 100)
        if percent >= self.next_percent:
            status("Processed %d%% " % self.next_percent, with_ts=True)
            self.next_percent += 5


def drain(progress: Progress, u, prog_cb: ProgressCb.cb, dst: str, tok_secret_name: str):
    r = http_get(u, headers={
        "OSF-TOKEN": secret(tok_secret_name),
        "Connection": "keep-alive",
        "Keep-Alive": "timeout=1200, max=1"  # keep connection alive for 1 request for 20m
    }, stream=True)
    last_pos = 0
    with io.BufferedReader(r.raw, buffer_size=1024*1024) as br:
        with tarfile.open(fileobj=br, mode="r|bz2") as tar_stream:
            for member in tar_stream:
                tar_stream.extract(member, dst)
                prog_cb(br.tell() - last_pos)
                last_pos = br.tell()
    status(f"Downloaded and extracted: {u}")
    progress.tick()


def _download_extract(progress: Progress, tarurls: List[str], dst: str, tok_secret_name: str):
    status(f"Downloading and extracting {len(tarurls)} ostree commits: {tarurls} -> {dst}ostree_repo")
    total_length = 0
    for u in tarurls:
        r = requests.head(u, headers={"OSF-TOKEN": secret(tok_secret_name)}, allow_redirects=True)
        if r.status_code != 200:
            sys.exit('Unable to find %s: %d\n%s' % (u, r.status_code, r.text))

        total_length += int(r.headers["content-length"])
        r.org_url = u

    cb = ProgressCb(total_length)

    # Since processes are not spawned in the workers we can use more workers, so we don't
    # to keep connection with GCS alive until one of the workers starts reading data from it.
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = []
        for u in tarurls:
            futures.append(executor.submit(drain, progress, u, cb.cb, dst, tok_secret_name))
        for f in futures:
            f.result()


def pull_ostree_commit(factory: str, commit_hash: str, ostree_repo_dir: str, tok_secret_name: str,
                       base_url: str):
    auth_url = f"{base_url}/{factory}/v2/repos/lmp/download-urls"
    r = requests.post(auth_url, headers={"osf-token": secret(tok_secret_name)})
    r.raise_for_status()

    pull_base_url = r.json()[0]["download_url"]
    pull_token = r.json()[0]["access_token"]

    if not os.path.exists(os.path.join(ostree_repo_dir, "config")):
        cmd("ostree", "init", "--repo", ostree_repo_dir, "--mode", "archive")

    cmd("ostree", "remote", "add", "--force", "--repo", ostree_repo_dir,
        "--no-gpg-verify", "gcs", pull_base_url)

    def read_progress(fd):
        data = os.read(fd, 70)
        line = data.decode()
        start_indx = line.find("Receiving")
        if start_indx == -1:
            return "\n".encode("ascii")
        res = "|--" + line[start_indx:].rstrip()
        sys.stdout.buffer.flush()
        return res.replace('\xa0', "%").encode("ascii")

    pty.spawn(["ostree", "pull", "--repo", ostree_repo_dir, "--update-frequency=5000",
               f"--http-header=Authorization=Bearer {pull_token}", "gcs", commit_hash],
              read_progress)


def upload_delta_stats(progress: Progress, factory: str, delta_stats: dict, tok_secret_name: str):
    ostreehub_uri = f"https://api.foundries.io/ota/ostreehub/{factory}/v2/repos/lmp/delta-stats"
    for to_sha, s in delta_stats.items():
        status(f"Uploading delta stats for {to_sha}...")
        r = requests.put(ostreehub_uri,
                         headers={
                             "osf-token": secret(tok_secret_name),
                             "content-type": "application/json"
                         },
                         params={"sha256": s["sha256"]},
                         data=s["canonical-json"])
        if not r.ok:
            raise requests.exceptions.HTTPError("Failed to upload delta stats to ostreehub; "
                                                f"{r.status_code}, err: {r.text}")
        # make sure the `sha` and `size` received from ostreehub matches with the original one
        if r.json()["sha256"] != s["sha256"]:
            raise Exception("invalid content hash is received from ostreehub; "
                            f"expected: {s['sha256']}, received: {r.json()['sha256']}")
        if int(r.json()["size"]) != len(s["canonical-json"]):
            raise Exception("invalid content size is received from ostreehub; "
                            f"expected: {len(s['canonical-json'])}, received: {r.json()['size']}")
        progress.tick()


def add_delta_stat_refs_to_targets(creds_zip_file: str, delta_stats: dict):
    status("Updating Factory TUF Targets with references to delta statistics...")
    tur_repo_path = "./tuf-repo"
    cmd("garage-sign", "init", "--repo", tur_repo_path, "--credentials", creds_zip_file)
    cmd("garage-sign", "targets", "pull", "--repo", tur_repo_path)
    targets_file = f"./tuf/{tur_repo_path}/roles/unsigned/targets.json"
    with open(targets_file) as f:
        targets = json.load(f)

    for target_name, target_value in targets["targets"].items():
        if target_value["hashes"]["sha256"] in delta_stats:
            target_value["custom"]["delta-stats"] = {
                "sha256": delta_stats[target_value["hashes"]["sha256"]]["sha256"],
                "size": len(delta_stats[target_value["hashes"]["sha256"]]["canonical-json"])
            }

    with open(targets_file, "w") as f:
        json.dump(targets, f)

    cmd("garage-sign", "targets", "sign", "--repo", tur_repo_path, "--key-name", "targets")
    cmd("garage-sign", "targets", "push", "--repo", tur_repo_path)


def main(creds_zip_file: str, deltas: List[Delta], factory: str, tok_secret_name: str, out_dir: str):
    load_extra_certs()

    work = 1  # 1 for the fiopush
    downloads = []
    pulls = []
    for d in deltas:
        downloads.append(d.to[1]) if d.to[1] else pulls.append(d.to[0])
        work += 1
        for x in d.froms:
            if x[1]:
                downloads.append(x[1])
            else:
                pulls.append(x[0])
            # 2 for the "from":  download | pull, and generate delta
            work += 2
        # 2 for the "to":
        #   1. storing the delta stats to a file as a CI artifact
        #   2. uploading to ostreehub
        work += 2
    # updating targets
    work += 1

    base = fio_dnsbase()
    ostree_url = f"https://api.{base}/ota/ostreehub"

    prog = Progress(work)
    if len(downloads) > 0:
        _download_extract(prog, downloads, "./", tok_secret_name)

    if len(pulls) > 0:
        status(f"Pulling: {pulls} -> ./ostree_repo")
        for commit in pulls:
            pull_ostree_commit(factory, commit, "./ostree_repo", tok_secret_name, ostree_url)

    # # update summary and generate a new type of delta indexes
    cmd("ostree", "summary", "-u", "--repo=./ostree_repo")

    # TODO: remove the following code that determines and prints a type of delta indexes been generated
    #  once we completely switch to the new indexes types. https://foundriesio.atlassian.net/browse/FFTK-1122
    if os.path.isdir("./ostree_repo/delta-indexes"):
        with open("./ostree_repo/config", "r") as f:
            config = f.readlines()
        for e in config:
            if e == "indexed-deltas=true\n":
                status("New type of delta indexes are generated and enabled in the repo config")
                break
    else:
        status("Old type of delta indexes are generated (in-summary type)")

    delta_stats = generate_deltas(prog, deltas, "./ostree_repo")
    rmtree("./ostree_repo/objects")  # We just need to sync deltas
    os.remove("./ostree_repo/summary")  # summary is generated by the ostree server
    cmd("fiopush", "-summary", "-repo=./ostree_repo", "-creds", creds_zip_file)
    prog.tick()

    for to_sha, s in delta_stats.items():
        with open(os.path.join(out_dir, f"{to_sha}.json"), "wb") as f:
            f.write(s["canonical-json"])
        prog.tick()

    upload_delta_stats(prog, factory, delta_stats, tok_secret_name)
    add_delta_stat_refs_to_targets(creds_zip_file, delta_stats)
    prog.tick()


if __name__ == "__main__":
    require_env("FACTORY")
    require_secrets("osftok", "triggered-by", "deltas", "targets.sec", 'root.json', 'targets.pub')
    _, creds_tmp = mkstemp()
    generate_credential_tokens(creds_tmp)

    deltas: List[Delta] = []
    for d in json.loads(secret("deltas")):
        deltas.append(Delta(**d))

    factory = os.environ["FACTORY"]
    repo_parent = os.environ.get("OSTREE_REPO_ROOT", "/")
    os.chdir(repo_parent)
    main(creds_tmp, deltas, factory, "osftok", os.environ.get("ARCHIVE", "/archive"))
